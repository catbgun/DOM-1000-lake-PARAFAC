---
title: "DOM-1000-lake-PARAFAC"
author: "CBG"
date: "9 11 2021"
output: html_document
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# An Exploration of Fluorescence EEM PARAFAC - "1000 Lakes Study"
A subset of the samples from the 1000-lakes survey was analysed for fluroescence and emission excitation matrices obtained. The data was analysed using PARAFAC, producing three components that we are confident. In here, the components and other variables are explored. 

```{r load required packages, include=FALSE}
Packages <- c("data.table", "dplyr", "tidyr", "psych",
              "ggplot2", "ggpubr", "tidyverse", "broom", "AICcmodavg", 
              "cowplot", "googleway", "ggrepel","ggspatial",
                    "sf", "rnaturalearth", "rnaturalearthdata",
              "stats")
#"libwgeom",
lapply(Packages, library, character.only = TRUE)
```

```{r Functions needed below}
give.n <- function(x){
  return(c(y = median(x)*1.05, label = length(x))) 
  # experiment with the multiplier to find the perfect position
}
```

### The following datasets will be required
- EEM sample list
- UV-Vis absorbance data 
- chemistry + catchment data
- sample ID conversion file for the file above
- PARAFAC components
- Fluorescence indices
- Biodegradation data (UiO, Camille)

```{r load required data and merge the two, keep only the 501 samples}
EEMlist <- fread("EEM_sampleList.txt", sep = "\t", header = TRUE, select = c("sample"))
chemcatch <- read.table("1000 lakes joint 29102021_3.txt", sep = "\t", header = TRUE)
d1 <- read.table("1000 sjøer station id og kode.csv", sep = ";", header = TRUE)
abs <- read.table("A1000Lakes_all_trans.txt", sep = "\t", header = TRUE)
SNames <- fread("Provenavn.txt", sep = "\t", header = TRUE, select=c("Prøvenr", "Stasjonskode"))
comps <- fread("210823_Components.txt", sep = "\t", header = TRUE, select=c("sample", "Comp.1", "Comp.2", "Comp.3"))
fIndices <- read.table("210621_Indices_Smoothed.txt", sep = "\t", header = TRUE)
bio <- read.table("210623_UiO_Biodeg_V02.txt", sep = "\t", header = TRUE)
```

### Corrections that must be made includes
- UV-Vis correction
- column names adjusted for merging
```{r UV-Vis correction step, subtracting the mean}
abs$corr = rowMeans(subset(abs, select = c(252:277)))
absC <- sweep(abs[,c(2:876)], 1, abs$corr, "-")
absC$sample = abs$sample

d1$station_id = d1$ï..Station.ID

SNames$Station.Code.x = SNames$Stasjonskode
SNames$sample = SNames$Prøvenr
```

### Merging the datasets
- EEM list and UV-Vis abs based on sample 
- chem/catchment data need first to be merged with conversion file
- going further only with data with PARAFAC-comps
- Note that when mering, the df listed first is the ruling (e.g. for comps data)
```{r merging datasets}
df1 <- merge(EEMlist,abs, by  = "sample") 
dfx <- merge(chemcatch, d1, by  = "station_id") 
df2 <- merge(dfx, SNames, by  = "Station.Code.x") 
df3 <- merge(df1, df2, by  = "sample") 
df4 <- merge(comps, df3, by  = "sample", all.x=TRUE) 
df5 <- merge(df4, fIndices, by  = "sample", all.x=TRUE) 
df6 <- merge(df5, bio, by  = "sample", all.x=TRUE) 
df7 <- df6 %>% 
  drop_na(Comp.1)
```
### Calculating the following new variables
- TOTC:TOTN
- TOTC:TOTP???
- absorbency indexes: sUVa, sVISa, SAR, E2_E3, HI
- absorbency slope ratios???
- components, relative (%) and by C
- biodeg rates by C
- fluorescence peaks by C
- metal concentrations by C
```{r Calculating new variables}
dx1 <- transform(df7, TOC_TON = (X2019_TOC / X2019_TON))
dx2 <- transform(dx1, sUVa = ((X254.3999939/X2019_DOC.C)*100))
dx3 <- transform(dx2, sVISa = ((X410.3999939/X2019_DOC.C)*1000))
dx4 <- transform(dx3, SAR = (X254.3999939/X410.3999939))
dx5 <- transform(dx4, E2_E3 = (X254.3999939/X364.7999878))
dx6 <- transform(dx5, HI = (X284.7999878/X254.3999939))
#spectral slopes
dy6 <- transform(dx6, A220_254 = (X219.1999969/X254.3999939)) #polarity
dy7 <- transform(dy6, A250_364 = (X249.5999908/X364.7999878))#size/weight
dy8 <- transform(dy7, A300_400 = (X300/X400)) #humification
#the following two should also be included, but require curve fitting. See Maeve script.
#S275-295 #photodegradation, Helms et al
#SR

#Maeve: Extracting slopes
#Slope275_295 <-Slope275_295 %>%
#group_by(Sample) %>%
#do({mod = lm(Y ~ X, data =.)
#data_frame(Slope275_295 = abs(coef(mod)[2]))})
#Slope350_400 <-Slope350_400 %>%
#group_by(Sample) %>%
#do({mod = lm(Y ~ X, data =.)
#data_frame(Slope350_400 = abs(coef(mod)[2]))})
#Slopes<- full_join(Slope350_400,Slope275_295) %>%
#mutate(SR=Slope275_295/Slope350_400) %>%
#print(n=5)

dx7 <- transform(dy8, Comp1.R = (Comp.1 / (Comp.1+Comp.2+Comp.3))*100)
dx8 <- transform(dx7, Comp2.R = (Comp.2 / (Comp.1+Comp.2+Comp.3))*100)
dx9 <- transform(dx8, Comp3.R = (Comp.3 / (Comp.1+Comp.2+Comp.3))*100)

dx10 <- transform(dx9, Comp1.C = (Comp.1 / X2019_DOC.C))
dx11 <- transform(dx10, Comp2.C = (Comp.2 / X2019_DOC.C))
dx12 <- transform(dx11, Comp3.C = (Comp.3 / X2019_DOC.C))

#Make Biodeg parameters:DOC
dx13 <- transform(dx12, dmax.C = (dmax / X2019_DOC.C))
dx14 <- transform(dx13, auc.C = (auc / X2019_DOC.C))
dx15 <- transform(dx14, hauc.C = (hauc / X2019_DOC.C))
dx16 <- transform(dx15, hwidth.C = (hwidth / X2019_DOC.C))

#Fluorescence peaks a, m, c C-normalised
dx17 <- transform(dx16, a.C = (a / X2019_DOC.C))
dx18 <- transform(dx17, m.C = (m / X2019_DOC.C))
dx19 <- transform(dx18, c.C = (c / X2019_DOC.C))

#Others identified by Heleen
dz20 <- transform(dx19, NO3.TOTP = (X2019_NO3.N/ X2019_TOTP))
dz21 <- transform(dz20, TON.TOC = (X2019_TON/ X2019_TOC))

#Metals C-normalised
dx20 <- transform(dz21, Al.C = (X2019_Al / X2019_DOC.C))
dx21 <- transform(dx20, Cd.C = (X2019_Cd / X2019_DOC.C))
dx22 <- transform(dx21, Co.C = (X2019_Co / X2019_DOC.C))
dx23 <- transform(dx22, Cr.C = (X2019_Cr / X2019_DOC.C))
dx24 <- transform(dx23, Fe.C = (X2019_Fe / X2019_DOC.C))
dx25 <- transform(dx24, Hg.C = (X2019_Hg / X2019_DOC.C))
dx26 <- transform(dx25, Pb.C = (X2019_Pb/ X2019_DOC.C))
dx27 <- transform(dx26, Mn.C = (X2019_Mn/ X2019_DOC.C))
```

## Making summary tables and boxplots
- summary table of desired variables
- can be grouped by region
- select some catchment properties, chemical parameters influencing fluorescence
- include A340+A275

```{r Summary table}
describe(dx27[ , c('X2019_pH', 'X2019_DOC.C', "forest", "peat", "freshwater",
                   "del_SO4")], fast=TRUE)
nrow(dx27)

describe(dx27[ , c('Region.4', 'X2019_DOC.C', "lake_area_km2","lake_catchm_.", "X2019_pH", "X2019_Fe", "X2019_NO3.N", "forest", "peat", "freshwater",
                   "del_SO4")], fast=TRUE)
```

## Grouping data based on grouping variables to look for noamrlity etc.
```{r grouping data}
Group1 <- subset(dx27, Region.4 == "1-South",
select=c(Comp1.R, Comp2.R, Comp3.R))
Group2 <- subset(dx27, Region.4 == "2-East",
select=c(Comp1.R, Comp2.R, Comp3.R))
Group3 <- subset(dx27, Region.4 == "3-Mountains",
select=c(Comp1.R, Comp2.R, Comp3.R))
Group4 <- subset(dx27, Region.4 == "4-West",
select=c(Comp1.R, Comp2.R, Comp3.R))
Group5 <- subset(dx27, Region.4 == "5-Central",
select=c(Comp1.R, Comp2.R, Comp3.R))
Group6 <- subset(dx27, Region.4 == "6-North",
select=c(Comp1.R, Comp2.R, Comp3.R))
```

## Testing the groped observations
- normality of distribution (density plots and Q-Q plot, and Shapiro-Wilk normality test)
- homogeneity of variance
- independence of observations
-> the data was not normally distributed. Kruskil-Wallis test instead of ANOVA?
```{r density plot and Q-Q plot for normality validatio}
library("ggpubr")
ggdensity(Group1$Comp3.R, 
          main = "Density plot of tooth length",
          xlab = "Tooth length")

ggqqplot(dx27$Comp1.R)
shapiro.test(Group1$Comp1.R)
```
## Statistical testing between, e.g. the regional groups
- ANOVA: H0: no significant difference between the groups
- One-way for component distribution and region
- If p < 0.05 use Tukey test (t-test that adjusts for multiple comparisons) to find which group that is significantly different from the others
- Tukey test (H0: two means are equal), if p-adj for pair is < 0.05 

we see that there is significant difference among the region groups for all three components (rel %).
Further, significant differences are identifyed between the follwoing groups for each of the components:
Comp1: mountains-south, north-south, mountains-east, north-east, celntral-mountains, north-west, north-central
Comp2: east-south, mountains-south, west-south, north-south, mountains-east, north-east, west-mountains, north-west, north-central
Comp3: east-south, mountains-south, west-south, central-south, north-south, mountains-east, west-east, north-east, west-mountains, central-mountains, north-west, north-central

```{r t-testing of rel-comps between regions}
one.way1 <- aov(Comp1.R ~Region.4 , data = dx27)
one.way2 <- aov(Comp2.R ~Region.4 , data = dx27)
one.way3 <- aov(Comp3.R ~Region.4 , data = dx27)
summary(one.way1)
summary(one.way2)
summary(one.way3)

TukeyHSD(one.way1, conf.level=.95)
TukeyHSD(one.way2, conf.level=.95)
TukeyHSD(one.way3, conf.level=.95)

ggplot(dx27, aes(x=Region.4, y=Comp1.R, fill=Region.4)) + 
    geom_boxplot()+
  stat_summary(fun.data = give.n, geom = "text", fun = median,
                  position = position_dodge(width = 0.75))

ggplot(dx27, aes(x=Region.4, y=Comp2.R, fill=Region.4)) + 
    geom_boxplot()+
  stat_summary(fun.data = give.n, geom = "text", fun = median,
                  position = position_dodge(width = 0.75))

ggplot(dx27, aes(x=Region.4, y=Comp3.R, fill=Region.4)) + 
    geom_boxplot()+
  stat_summary(fun.data = give.n, geom = "text", fun = median,
                  position = position_dodge(width = 0.75))
```

```{r Which samples are included/excluded?}
# plot map with lakes
# find cuoff e.g. at DOC/Abs
co <- merge(comps, df3, by  = "sample", all=TRUE) 

co2 <- subset(co, select = c("sample", "X2019_DOC.C", "Comp.1", "Region.4"))
summary(co2)
exc <- subset(co, is.na(Comp.1))
exc2 <- subset(exc, select = c("sample", "X2019_DOC.C", "Comp.1", "X2019_NO3.N", "X2019_Fe", "Region.4"))
exc3 <- transform(exc2, Fe.C = (X2019_Fe / X2019_DOC.C))

table(exc$Region.4)
nrow(exc)

#Who are the low DOC excluded samples? wherew are they coming from?
new_df <- subset(exc3, !(sample %in% c("DOM13465", "DOM12725", "DOM13458", "DOM13474", "DOM13294", "DOM11828", "DOM11861", "DOM11825", "DOM11824", "DOM11766", "DOM12209", "DOM13331", "DOM11314", "DOM11394", "DOM11392", "DOM11372")))

nrow(new_df)
table(new_df$Region.4)               
nrow(exc3)
```

```{r MAPPING}
world <- ne_countries(scale = "medium", returnclass = "sf")
ggplot(data = world, zoom=4) +
  geom_sf()+
  coord_sf(xlim = c(2, 30), ylim = c(58, 71))+
  geom_point(aes(x = longitude, y = latitude, colour=Region.4), 
             data=dx27, size =2)+
  labs(title = "", x = "", y = "")+
  theme(axis.text.x = element_text(angle=0, vjust=1, size = 12),
        axis.text.y = element_text(size= 12),
        legend.title = element_text(size = 18),
        axis.title.x = element_text(size = 13, margin=margin(13,0,0,0)),
        axis.title.y = element_text(size = 13, margin=margin(0,13,0,0)),
        legend.text=element_text(size=16),
        plot.title = element_text(size = 24),
        legend.key = element_rect(fill = "black", color = NA),
        panel.background=element_rect(fill = "white", colour = "white"),
        panel.border = element_rect(colour = "grey", fill=NA))
 # scale_color_gradient(low="green", high="red", na.value="white")+
  #labs(color = "Comp1 (%)")+
  #ggtitle("Microbial humic-like (autochthonous), Comp3 (%)))")
#labs(color = expression(paste("pCO"["2"], "(uatm)")))

#expression(paste("pCO"["2"] "(",mu,"atm") 
#(y=expression(Blah[1][d]))

#ggsave("Fig1_PB.png", plot=last_plot(), device=png)
#scale_colour_manual(values=c("deeppink2", "orchid1","turquoise1",  "turquoise3","seashell4"))
```
## PCA Explorative phase
https://www.r-bloggers.com/2013/11/computing-and-visualizing-pca-in-r/
https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/
- first look at correlations, include those over 0.3 for explorative PCA
https://www.researchgate.net/post/What_are_the_criteria_for_selecting_variables_for_PCA_analysis
- include those not included in the beginning
- log transform?
- To ensure assumptions of the model (PCA and PLS) were met,
data were normalized using log-transformations where necesssary, with skewness targeted to a min/max <0.1 
- Since skewness and the magnitude of the variables influence the resulting PCs, it is good practice to apply skewness transformation, center and scale the variables prior to the application of PCA.
- The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings)

You may use PC scores to reduce variables (i.e. the correlated ones). firstly, you should do a PCA on your parameters using predetermined number of PCs (lets say N= 10 PCs). Then check to see how many PCs are needed to describe 100% cumulative %variation (for example the first 5 out of 10 PCs). In each PC (1st to 5th) choose the variable with the highest score (irrespective of its positive or negative sign) as the most important variable. Since PCs are orthogonal in the PCA, selected variables will be completely independent (non-correlated).

Note: Partial least square (PLS) is a supervised alternative to PCA. PLS assigns higher weight to variables which are strongly related to response variable to determine principal components.

ISSUE WITH NAS IN DATA, PARTICULARLY IN DELTAS, CAUSING ISSUE FOR PCA. For this initial testing either exclude the parameters or the observations. Need to figure out where these NAs come from!!!!
IF removing NAs, issue with categorical variable Regions. needs to remove NAs before separating the two.
-ANCorg with neg values cannot be log transformed
```{r Preparation for PCA: variable selection and Log-transform}
#First test with only 2019 data without spectroscopica data
pca1 <- subset(dx27, select=c(Region.4, latitude,	longitude,	altitude,	lake_area_km2,	basin_area_km2,	lake_catchm_.,	min_elev,	mean_elev,	max_elev,	developed,	agriculture,	forest,	natural_not_forest,	peat,	glacier,	freshwater,	Avrenning,	ERA5.mean.annual.temperature.2019,	ERA5.total.annual.precipitation.2019,	ERA5.total.annual.runoff.2019,	X2019_ALK,	X2019_Al.Il,	X2019_Al.R,	X2019_Al,	X2019_As,	X2019_Ca,	X2019_Cd,	X2019_Cl,	X2019_Co,	X2019_Cr,	X2019_Cu,	X2019_DOC.C,	X2019_FARGE,	X2019_F,	X2019_Fe,	X2019_Hg,	X2019_KOND,	X2019_K,	X2019_Mg,	X2019_Mn,	X2019_NH4.N,	X2019_NO3.N,	X2019_Na,	X2019_Ni,	X2019_PO4.P,	X2019_Pb,	X2019_SO4,	X2019_SiO2,	X2019_TOC,	X2019_CNDOM,	X2019_TOTN,	X2019_TON,	X2019_TOTP,	X2019_Zn,	X2019_pH,	X2019_H,	X2019_LAL,	X2019_ALKE,	X2019_ANC, TOC_TON, Cd.C, Co.C, Cr.C, Fe.C, Hg.C, Pb.C, Mn.C))

pca1x <- pca1 %>% 
  mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
pca1y <- subset(pca1x, select=-c(Region.4)) 
#Logtransform parameters and change names to "_log"
ell <- log(pca1y+15)
names(ell) <- paste0(names(ell), "_log")

#pca2 <- subset(dx27, select=c(Region.4, latitude,	longitude,	altitude,	lake_area_km2,	basin_area_km2,	lake_catchm_.,	min_elev,	mean_elev,	max_elev,	developed,	agriculture,	forest,	natural_not_forest,	peat,	glacier,	freshwater,	Avrenning,	ERA5.mean.annual.temperature.2019,	ERA5.total.annual.precipitation.2019,	ERA5.total.annual.runoff.2019,	X2019_ALK,	X2019_Al,	X2019_Ca,	X2019_Cl,	X2019_DOC.C,	X2019_FARGE,	X2019_F,	X2019_Fe,	X2019_Hg,	X2019_KOND,	X2019_K,	X2019_Mg,	X2019_Mn,	X2019_NH4.N,	X2019_NO3.N,	X2019_Na,	X2019_PO4.P,	X2019_SO4,	X2019_SiO2,	X2019_TOC,	X2019_CNDOM,	X2019_TOTN,	X2019_TON,	X2019_TOTP,	X2019_pH,	X2019_H,	X2019_LAL,	X2019_ALKE,	X2019_ANC, TOC_TON, Cd.C, Co.C, Cr.C, Fe.C, Hg.C, Pb.C, Mn.C))
 
#impute missing values with median
library(tidyverse)
#pca2x <- pca2 %>% 
  mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
#pca2y <- subset(pca2x, select=-c(Region.4))                           
                            
#Logtransform parameters and change names to "_log"
#ell <- log(pca2y+15)
#names(ell) <- paste0(names(ell), "_log")

pca3 <- subset(dx27, select=c(Region.4, Comp1.R, Comp2.R, Comp3.R, latitude,	altitude,	mean_elev,	max_elev,	agriculture,	forest,	natural_not_forest,	Avrenning,	ERA5.mean.annual.temperature.2019,	ERA5.total.annual.precipitation.2019,	ERA5.total.annual.runoff.2019,	X2019_ALK,	X2019_Al,	X2019_Ca,	X2019_Cl,	X2019_DOC.C,	X2019_FARGE,	X2019_KOND,	X2019_Mg,	X2019_NO3.N,	X2019_Na,	X2019_PO4.P,	X2019_SO4,	X2019_SiO2,	X2019_CNDOM,	X2019_TOTN,	X2019_TON,	X2019_TOTP,	X2019_H,	X2019_LAL,	X2019_ANC, TOC_TON, Fe.C, Hg.C))
 
#impute missing values with median
library(tidyverse)
pca3x <- pca3 %>% 
  mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
pca3y <- subset(pca3x, select=-c(Region.4))                           
                            
#Logtransform parameters and change names to "_log"
ell <- log(pca3y+15)
names(ell) <- paste0(names(ell), "_log")

#Looking only at the DOM quality variables++
pca.DOM <- subset(dx27, select=c(Region.4, Comp1.R, Comp2.R, Comp3.R, sUVa, sVISa, E2_E3, HI, A220_254, A250_364, A300_400, dmax.C, a.C, m.C, c.C, NO3.TOTP, TON.TOC,
                                 latitude,	altitude,	mean_elev,	max_elev,	agriculture,	forest,	natural_not_forest,	Avrenning,	ERA5.mean.annual.temperature.2019,	ERA5.total.annual.precipitation.2019,	ERA5.total.annual.runoff.2019,
                                 X2019_ALK,	X2019_Al, X2019_Cl,	X2019_DOC.C,	X2019_KOND, X2019_SO4, X2019_CNDOM, Fe.C, Hg.C, X2019_H))

pca.DOMx <- pca.DOM %>% 
  mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
pca.DOMy <- subset(pca.DOMx, select=-c(Region.4))                          
              
ell <- log(pca.DOMy+15)
names(ell) <- paste0(names(ell), "_log")


```
to make a selection of which parameters to include: redundancy analysis

```{r Explore correlations}
library(stats)
#library(ggfortify)
#library(devtools)
#install_github("ggbiplot", "vqv")

#library(devtools)
library(ggbiplot)
library(vqv)
#install_github("vqv/ggbiplot")

library(devtools)
install_github("vqv/ggbiplot")

#pca1x <- na.omit(pca1y)
elli <- na.omit(ell)
mtcars.pca <- prcomp(elli, scale. = TRUE, center=TRUE)

biplot(mtcars.pca)
options(max.print=1000000)
print(mtcars.pca)
summary(mtcars.pca)
plot(mtcars.pca)


#chart.Correlation(chemistry_metals10, histogram=TRUE, pch=19)
```


```{r PCA nice plotting}
# http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

library("devtools")
install_github("kassambara/factoextra")

library("factoextra")

fviz_pca_biplot(mtcars.pca, select.var = list(contrib = 25),
                col.ind = pca1$Region.4,  
                addEllipses = FALSE, label = "var",
                repel = TRUE,
                legend.title = "Species") 

fviz_pca_var(mtcars.pca, select.var = list(contrib = 25))
             
             #select.var = list(cos2 = 0.6))

```




```{r PCA with correlating parameters, p > 0.2}
# PCA of all parameters but optical 
head(df4)
dfpca <- df4[,c(2, 7, 10, 12:13, 15, 
              63:64, 66, 74, 89, 62,
              33, 38, 
              93, 96:99)]

mtcars.pca <- prcomp(na.omit(dfpca), scale. = TRUE)

```



